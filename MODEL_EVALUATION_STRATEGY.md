# FaceWatch ëª¨ë¸ í‰ê°€ ì „ëµ

## ğŸ“‹ ëª©ì°¨

1. [ê°œìš”](#1-ê°œìš”)
2. [í‰ê°€ ë°ì´í„° ì¤€ë¹„](#2-í‰ê°€-ë°ì´í„°-ì¤€ë¹„)
3. [í‰ê°€ ì§€í‘œ](#3-í‰ê°€-ì§€í‘œ)
4. [í‰ê°€ êµ¬í˜„](#4-í‰ê°€-êµ¬í˜„)
5. [ì„ê³„ê°’ ìµœì í™”](#5-ì„ê³„ê°’-ìµœì í™”)
6. [ê²°ê³¼ í•´ì„](#6-ê²°ê³¼-í•´ì„)
7. [ì‹¤ì „ ì˜ˆì‹œ](#7-ì‹¤ì „-ì˜ˆì‹œ)

---

## 1. ê°œìš”

### 1.1 í‰ê°€ ëª©ì 

FaceWatch ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ê²°ê³¼ì˜ **ì‹ ë¢°ë„**ë¥¼ ì •ëŸ‰ì ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤.

**í•µì‹¬ ì§ˆë¬¸**:
- âœ… ëª¨ë¸ì´ "96%"ë¼ê³  ì˜ˆì¸¡í•˜ë©´ ì‹¤ì œë¡œ 96% ì •í™•í•œê°€?
- âœ… "50%"ë¡œ ì˜ˆì¸¡í•œ ê²ƒì€ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ”ê°€?
- âœ… ì–´ë–¤ ì„ê³„ê°’(threshold)ì„ ì‚¬ìš©í•´ì•¼ ìµœì ì¸ê°€?

### 1.2 ì™œ í¬ë¡œìŠ¤ì—”íŠ¸ë¡œí”¼ê°€ ì•„ë‹Œê°€?

| í¬ë¡œìŠ¤ì—”íŠ¸ë¡œí”¼ | FaceWatch (ì½”ì‚¬ì¸ ìœ ì‚¬ë„) |
|---------------|--------------------------|
| **Closed-set**: ê³ ì •ëœ Nê°œ í´ë˜ìŠ¤ | **Open-set**: "unknown" í¬í•¨ |
| **í™•ë¥  ë¶„í¬**: í•©=1.0 í•„ìš” | **ìœ ì‚¬ë„**: í•©â‰ 1.0 OK |
| **í•™ìŠµìš©**: ëª¨ë¸ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ | **ì¶”ë¡ ìš©**: ì´ë¯¸ í•™ìŠµ ì™„ë£Œ |

**ê²°ë¡ **: Open-set ì–¼êµ´ ì¸ì‹ì—ëŠ” **ë‹¤ë¥¸ í‰ê°€ ì§€í‘œ** í•„ìš”

---

## 2. í‰ê°€ ë°ì´í„° ì¤€ë¹„

### 2.1 Ground Truth ë¼ë²¨ë§

#### ë¼ë²¨ë§ í˜•ì‹

```json
{
  "frame_001.jpg": "yh",
  "frame_002.jpg": "ja",
  "frame_003.jpg": "unknown",
  "frame_004.jpg": "yh",
  "frame_005.jpg": "js"
}
```

#### ë¼ë²¨ë§ ê°€ì´ë“œ

| ë¼ë²¨ | ì„¤ëª… | ì˜ˆì‹œ |
|------|------|------|
| `"yh"` | ëª…í™•íˆ yhë¡œ ì‹ë³„ ê°€ëŠ¥ | ì •ë©´, ì„ ëª…í•œ ì–¼êµ´ |
| `"ja"` | ëª…í™•íˆ jaë¡œ ì‹ë³„ ê°€ëŠ¥ | ì¸¡ë©´ì´ì§€ë§Œ ì‹ë³„ ê°€ëŠ¥ |
| `"unknown"` | ì‹ë³„ ë¶ˆê°€ëŠ¥ ë˜ëŠ” ë“±ë¡ë˜ì§€ ì•Šì€ ì¸ë¬¼ | ë’·ëª¨ìŠµ, íë¦¼, ë‹¤ë¥¸ ì‚¬ëŒ |
| `"ambiguous"` (ì„ íƒ) | ì• ë§¤í•œ ê²½ìš° | ê²½ê³„ ì¼€ì´ìŠ¤ |

#### ë¼ë²¨ë§ ë„êµ¬ (ê°„ë‹¨í•œ ìŠ¤í¬ë¦½íŠ¸)

```python
# scripts/label_ground_truth.py
import cv2
import json
from pathlib import Path

def label_frames(frames_dir, output_json):
    """
    í”„ë ˆì„ ì´ë¯¸ì§€ë¥¼ ë³´ì—¬ì£¼ê³  ì‚¬ëŒì´ ì§ì ‘ ë¼ë²¨ë§
    """
    frames = sorted(Path(frames_dir).glob("*.jpg"))
    labels = {}
    
    print("ë¼ë²¨ë§ ê°€ì´ë“œ:")
    print("  yh, ja, js, jw: í•´ë‹¹ ì¸ë¬¼")
    print("  unknown: ì‹ë³„ ë¶ˆê°€")
    print("  skip: ì´ í”„ë ˆì„ ê±´ë„ˆë›°ê¸°")
    print("  quit: ì¢…ë£Œ")
    
    for frame_path in frames:
        img = cv2.imread(str(frame_path))
        cv2.imshow("Frame", img)
        cv2.waitKey(1)
        
        label = input(f"{frame_path.name}: ").strip()
        
        if label == "quit":
            break
        elif label == "skip":
            continue
        else:
            labels[frame_path.name] = label
    
    cv2.destroyAllWindows()
    
    # JSON ì €ì¥
    with open(output_json, 'w', encoding='utf-8') as f:
        json.dump(labels, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… ë¼ë²¨ë§ ì™„ë£Œ: {len(labels)}ê°œ í”„ë ˆì„")
    print(f"ì €ì¥ ìœ„ì¹˜: {output_json}")

if __name__ == "__main__":
    label_frames(
        frames_dir="outputs/results/test_video/frames",
        output_json="outputs/evaluation/ground_truth.json"
    )
```

### 2.2 ëª¨ë¸ ì˜ˆì¸¡ ìˆ˜ì§‘

#### ì˜ˆì¸¡ í˜•ì‹

```json
{
  "frame_001.jpg": {
    "predicted_id": "yh",
    "confidence": 0.96,
    "all_scores": {
      "yh": 0.96,
      "ja": 0.25,
      "js": 0.18
    }
  },
  "frame_002.jpg": {
    "predicted_id": "yh",
    "confidence": 0.50,
    "all_scores": {
      "yh": 0.50,
      "ja": 0.48,
      "js": 0.20
    }
  }
}
```

#### ì˜ˆì¸¡ ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸

```python
# scripts/collect_predictions.py
import json
import cv2
from pathlib import Path
from insightface.app import FaceAnalysis
from src.utils.gallery_loader import load_gallery, match_with_bank_detailed
from src.face_enroll import l2_normalize

def collect_predictions(frames_dir, gallery_dir, output_json):
    """
    í”„ë ˆì„ë“¤ì— ëŒ€í•œ ëª¨ë¸ ì˜ˆì¸¡ ìˆ˜ì§‘
    """
    # ëª¨ë¸ ë¡œë“œ
    app = FaceAnalysis(name="buffalo_l")
    app.prepare(ctx_id=0, det_size=(640, 640))
    
    # ê°¤ëŸ¬ë¦¬ ë¡œë“œ
    gallery = load_gallery(gallery_dir, use_bank=True)
    
    predictions = {}
    frames = sorted(Path(frames_dir).glob("*.jpg"))
    
    for frame_path in frames:
        img = cv2.imread(str(frame_path))
        faces = app.get(img)
        
        if len(faces) == 0:
            predictions[frame_path.name] = {
                "predicted_id": "no_face",
                "confidence": 0.0,
                "all_scores": {}
            }
            continue
        
        # ì²« ë²ˆì§¸ ì–¼êµ´ë§Œ ì‚¬ìš© (ì£¼ì¸ê³µ ê°€ì •)
        face = faces[0]
        embedding = l2_normalize(face.embedding.astype("float32"))
        
        # ëª¨ë“  ì¸ë¬¼ê³¼ ìœ ì‚¬ë„ ê³„ì‚°
        all_scores = {}
        for person_id, bank in gallery.items():
            if bank.ndim == 2:
                sims = bank @ embedding
                max_sim = float(sims.max())
            else:
                max_sim = float(bank @ embedding)
            all_scores[person_id] = max_sim
        
        # ìµœê³  ìœ ì‚¬ë„
        best_id = max(all_scores, key=all_scores.get)
        best_score = all_scores[best_id]
        
        predictions[frame_path.name] = {
            "predicted_id": best_id,
            "confidence": best_score,
            "all_scores": all_scores
        }
        
        print(f"{frame_path.name}: {best_id} ({best_score:.2f})")
    
    # JSON ì €ì¥
    with open(output_json, 'w', encoding='utf-8') as f:
        json.dump(predictions, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… ì˜ˆì¸¡ ìˆ˜ì§‘ ì™„ë£Œ: {len(predictions)}ê°œ í”„ë ˆì„")

if __name__ == "__main__":
    collect_predictions(
        frames_dir="outputs/results/test_video/frames",
        gallery_dir="outputs/embeddings",
        output_json="outputs/evaluation/predictions.json"
    )
```

---

## 3. í‰ê°€ ì§€í‘œ

### 3.1 ê¸°ë³¸ ë¶„ë¥˜ ì§€í‘œ

#### 3.1.1 Confusion Matrix

```
                Predicted
              yh   ja   js   unknown
Actual  yh   [10   1    0     2    ]
        ja   [ 2   8    1     1    ]
        js   [ 0   1    7     0    ]
     unknown [ 1   2    0    15    ]
```

**Python êµ¬í˜„**:
```python
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

def plot_confusion_matrix(y_true, y_pred, labels):
    cm = confusion_matrix(y_true, y_pred, labels=labels)
    
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=labels, yticklabels=labels)
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.title('Confusion Matrix')
    plt.tight_layout()
    plt.savefig('outputs/evaluation/confusion_matrix.png', dpi=300)
    plt.show()
```

#### 3.1.2 Precision, Recall, F1-Score

```python
from sklearn.metrics import classification_report

report = classification_report(y_true, y_pred, 
                                target_names=['yh', 'ja', 'js', 'unknown'])
print(report)
```

**ì¶œë ¥ ì˜ˆì‹œ**:
```
              precision    recall  f1-score   support

          yh       0.77      0.77      0.77        13
          ja       0.67      0.67      0.67        12
          js       0.88      0.88      0.88         8
     unknown       0.83      0.83      0.83        18

    accuracy                           0.78        51
   macro avg       0.79      0.79      0.79        51
weighted avg       0.78      0.78      0.78        51
```

### 3.2 ì‹ ë¢°ë„ ë³´ì • ì§€í‘œ (í•µì‹¬!)

#### 3.2.1 ECE (Expected Calibration Error)

**ëª©ì **: "96%"ë¼ê³  ì˜ˆì¸¡í•œ ê²ƒ ì¤‘ ì‹¤ì œë¡œ 96%ê°€ ë§ëŠ”ì§€ í™•ì¸

**ìˆ˜ì‹**:
```
ECE = Î£ (|avg_confidence - avg_accuracy| Ã— n_samples_in_bin) / n_total
```

**Python êµ¬í˜„**:
```python
import numpy as np

def compute_ece(confidences, correctness, n_bins=10):
    """
    Expected Calibration Error
    
    Args:
        confidences: np.array([0.96, 0.50, 0.85, ...])
        correctness: np.array([True, False, True, ...])
        n_bins: êµ¬ê°„ ê°œìˆ˜
    
    Returns:
        ece: float (0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ)
    """
    bin_boundaries = np.linspace(0, 1, n_bins + 1)
    ece = 0
    
    for i in range(n_bins):
        lower = bin_boundaries[i]
        upper = bin_boundaries[i + 1]
        
        # ì´ êµ¬ê°„ì— ì†í•˜ëŠ” ì˜ˆì¸¡
        in_bin = (confidences >= lower) & (confidences < upper)
        
        if np.sum(in_bin) > 0:
            avg_confidence = np.mean(confidences[in_bin])
            avg_accuracy = np.mean(correctness[in_bin])
            weight = np.sum(in_bin) / len(confidences)
            
            ece += np.abs(avg_confidence - avg_accuracy) * weight
    
    return ece

# ì‚¬ìš© ì˜ˆì‹œ
confidences = np.array([0.96, 0.50, 0.85, 0.42, 0.78])
correctness = np.array([1, 0, 1, 1, 0])  # 1=ë§ìŒ, 0=í‹€ë¦¼

ece = compute_ece(confidences, correctness)
print(f"ECE: {ece:.4f}")
# ECE < 0.05: ë§¤ìš° ì˜ ë³´ì •ë¨
# ECE < 0.10: ì˜ ë³´ì •ë¨
# ECE > 0.20: ê³¼ì‹  ë˜ëŠ” ê³¼ì†Œì‹ 
```

#### 3.2.2 Reliability Diagram

```python
def plot_reliability_diagram(confidences, correctness, n_bins=10):
    """
    ì‹ ë¢°ë„ ë‹¤ì´ì–´ê·¸ë¨
    
    y=x ì§ì„ ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì˜ ë³´ì •ë¨
    """
    bin_boundaries = np.linspace(0, 1, n_bins + 1)
    bin_centers = []
    bin_accs = []
    bin_confs = []
    
    for i in range(n_bins):
        lower = bin_boundaries[i]
        upper = bin_boundaries[i + 1]
        
        in_bin = (confidences >= lower) & (confidences < upper)
        
        if np.sum(in_bin) > 0:
            bin_centers.append((lower + upper) / 2)
            bin_accs.append(np.mean(correctness[in_bin]))
            bin_confs.append(np.mean(confidences[in_bin]))
    
    plt.figure(figsize=(8, 8))
    plt.plot([0, 1], [0, 1], 'k--', label='Perfect calibration')
    plt.plot(bin_confs, bin_accs, 'o-', label='Model')
    plt.xlabel('Confidence')
    plt.ylabel('Accuracy')
    plt.title('Reliability Diagram')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig('outputs/evaluation/reliability_diagram.png', dpi=300)
    plt.show()
```

### 3.3 ì„ê³„ê°’ ë¬´ê´€ ì§€í‘œ

#### 3.3.1 ROC-AUC (Binary Classification)

**íŠ¹ì • ì¸ë¬¼ ê²€ì¶œ í‰ê°€** (ì˜ˆ: yh vs not yh)

```python
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

def evaluate_person_detection(y_true, y_scores, person_id):
    """
    íŠ¹ì • ì¸ë¬¼ ê²€ì¶œ ì„±ëŠ¥ í‰ê°€
    
    Args:
        y_true: ["yh", "ja", "unknown", "yh", ...]
        y_scores: [0.96, 0.50, 0.30, 0.85, ...]
        person_id: "yh"
    """
    # Binaryë¡œ ë³€í™˜
    y_binary = [1 if label == person_id else 0 for label in y_true]
    
    # ROC ê³„ì‚°
    fpr, tpr, thresholds = roc_curve(y_binary, y_scores)
    roc_auc = auc(fpr, tpr)
    
    # í”Œë¡¯
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, label=f'{person_id} (AUC = {roc_auc:.3f})')
    plt.plot([0, 1], [0, 1], 'k--', label='Random')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'ROC Curve - {person_id}')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(f'outputs/evaluation/roc_{person_id}.png', dpi=300)
    plt.show()
    
    # ìµœì  ì„ê³„ê°’ (Youden's J)
    optimal_idx = np.argmax(tpr - fpr)
    optimal_threshold = thresholds[optimal_idx]
    
    print(f"{person_id} Detection:")
    print(f"  AUC: {roc_auc:.3f}")
    print(f"  Optimal Threshold: {optimal_threshold:.3f}")
    print(f"  TPR at optimal: {tpr[optimal_idx]:.3f}")
    print(f"  FPR at optimal: {fpr[optimal_idx]:.3f}")
    
    return roc_auc, optimal_threshold
```

#### 3.3.2 Precision-Recall Curve

```python
from sklearn.metrics import precision_recall_curve, average_precision_score

def plot_pr_curve(y_binary, y_scores, person_id):
    """
    Precision-Recall Curve
    """
    precision, recall, thresholds = precision_recall_curve(y_binary, y_scores)
    ap = average_precision_score(y_binary, y_scores)
    
    plt.figure(figsize=(8, 6))
    plt.plot(recall, precision, label=f'{person_id} (AP = {ap:.3f})')
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title(f'Precision-Recall Curve - {person_id}')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(f'outputs/evaluation/pr_{person_id}.png', dpi=300)
    plt.show()
    
    # F1 ìµœëŒ€í™” ì„ê³„ê°’
    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)
    best_idx = np.argmax(f1_scores)
    best_threshold = thresholds[best_idx] if best_idx < len(thresholds) else 0.5
    
    print(f"  Best F1 Threshold: {best_threshold:.3f}")
    print(f"  F1 at best: {f1_scores[best_idx]:.3f}")
    
    return ap, best_threshold
```

### 3.4 í™•ì‹ ë„ ë¶„í¬ ë¶„ì„

```python
def analyze_confidence_distribution(confidences, correctness):
    """
    í™•ì‹ ë„ êµ¬ê°„ë³„ ì •í™•ë„ ë¶„ì„
    """
    bins = [0, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0]
    labels = ['0-30%', '30-40%', '40-50%', '50-60%', 
              '60-70%', '70-80%', '80-100%']
    
    df = pd.DataFrame({
        'confidence': confidences,
        'correct': correctness
    })
    
    df['bin'] = pd.cut(df['confidence'], bins=bins, labels=labels)
    
    # êµ¬ê°„ë³„ í†µê³„
    stats = df.groupby('bin').agg({
        'correct': ['count', 'sum', 'mean']
    })
    
    stats.columns = ['Total', 'Correct', 'Accuracy']
    stats['Wrong'] = stats['Total'] - stats['Correct']
    
    print("Confidence Distribution Analysis:")
    print(stats)
    
    # ì‹œê°í™”
    stats['Accuracy'].plot(kind='bar', figsize=(10, 6))
    plt.ylabel('Accuracy')
    plt.xlabel('Confidence Range')
    plt.title('Accuracy by Confidence Range')
    plt.ylim(0, 1.0)
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig('outputs/evaluation/confidence_distribution.png', dpi=300)
    plt.show()
```

---

## 4. í‰ê°€ êµ¬í˜„

### 4.1 ì „ì²´ í‰ê°€ íŒŒì´í”„ë¼ì¸

```python
# scripts/evaluate_model.py
import json
import numpy as np
import pandas as pd
from pathlib import Path
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

class FaceWatchEvaluator:
    """
    FaceWatch ëª¨ë¸ í‰ê°€ í´ë˜ìŠ¤
    """
    
    def __init__(self, ground_truth_path, predictions_path, threshold=0.42):
        """
        Args:
            ground_truth_path: Ground truth JSON ê²½ë¡œ
            predictions_path: Predictions JSON ê²½ë¡œ
            threshold: ìœ ì‚¬ë„ ì„ê³„ê°’
        """
        # ë°ì´í„° ë¡œë“œ
        with open(ground_truth_path, 'r', encoding='utf-8') as f:
            self.ground_truth = json.load(f)
        
        with open(predictions_path, 'r', encoding='utf-8') as f:
            self.predictions = json.load(f)
        
        self.threshold = threshold
        
        # ê³µí†µ í”„ë ˆì„ë§Œ ì„ íƒ
        common_frames = set(self.ground_truth.keys()) & set(self.predictions.keys())
        self.frames = sorted(common_frames)
        
        # ë°ì´í„° ì •ë¦¬
        self._prepare_data()
    
    def _prepare_data(self):
        """ë°ì´í„° ì •ë¦¬"""
        self.y_true = []
        self.y_pred = []
        self.confidences = []
        self.correctness = []
        
        for frame in self.frames:
            gt = self.ground_truth[frame]
            pred_info = self.predictions[frame]
            
            pred_id = pred_info['predicted_id']
            confidence = pred_info['confidence']
            
            # ì„ê³„ê°’ ì ìš©
            if confidence < self.threshold:
                pred_id = 'unknown'
            
            self.y_true.append(gt)
            self.y_pred.append(pred_id)
            self.confidences.append(confidence)
            self.correctness.append(gt == pred_id)
        
        self.confidences = np.array(self.confidences)
        self.correctness = np.array(self.correctness)
    
    def evaluate_all(self, output_dir='outputs/evaluation'):
        """ì „ì²´ í‰ê°€ ì‹¤í–‰"""
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        print("="*70)
        print("FaceWatch ëª¨ë¸ í‰ê°€")
        print("="*70)
        print(f"í”„ë ˆì„ ìˆ˜: {len(self.frames)}")
        print(f"Threshold: {self.threshold}")
        print()
        
        # 1. ê¸°ë³¸ ë¶„ë¥˜ ì§€í‘œ
        self.evaluate_classification()
        
        # 2. ì‹ ë¢°ë„ ë³´ì •
        self.evaluate_calibration()
        
        # 3. ì¸ë¬¼ë³„ ROC/PR
        self.evaluate_per_person()
        
        # 4. í™•ì‹ ë„ ë¶„í¬
        self.evaluate_confidence_distribution()
        
        print("="*70)
        print("í‰ê°€ ì™„ë£Œ!")
        print(f"ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {output_dir}")
        print("="*70)
    
    def evaluate_classification(self):
        """ë¶„ë¥˜ ì„±ëŠ¥ í‰ê°€"""
        print("\n[1] Classification Metrics")
        print("-"*70)
        
        # Classification report
        labels = sorted(set(self.y_true) | set(self.y_pred))
        report = classification_report(self.y_true, self.y_pred, 
                                        target_names=labels, 
                                        zero_division=0)
        print(report)
        
        # Confusion matrix
        cm = confusion_matrix(self.y_true, self.y_pred, labels=labels)
        
        plt.figure(figsize=(10, 8))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=labels, yticklabels=labels)
        plt.ylabel('Actual')
        plt.xlabel('Predicted')
        plt.title(f'Confusion Matrix (Threshold = {self.threshold})')
        plt.tight_layout()
        plt.savefig('outputs/evaluation/confusion_matrix.png', dpi=300)
        plt.close()
        
        print("âœ… Confusion matrix saved")
    
    def evaluate_calibration(self):
        """ì‹ ë¢°ë„ ë³´ì • í‰ê°€"""
        print("\n[2] Calibration Metrics")
        print("-"*70)
        
        # ECE
        ece = self.compute_ece(self.confidences, self.correctness)
        print(f"Expected Calibration Error (ECE): {ece:.4f}")
        
        if ece < 0.05:
            print("  â†’ ë§¤ìš° ì˜ ë³´ì •ë¨ âœ…")
        elif ece < 0.10:
            print("  â†’ ì˜ ë³´ì •ë¨ âœ…")
        elif ece < 0.20:
            print("  â†’ ë³´í†µ âš ï¸")
        else:
            print("  â†’ ê³¼ì‹  ë˜ëŠ” ê³¼ì†Œì‹  ê²½í–¥ âŒ")
        
        # Reliability diagram
        self.plot_reliability_diagram()
        print("âœ… Reliability diagram saved")
    
    def compute_ece(self, confidences, correctness, n_bins=10):
        """ECE ê³„ì‚°"""
        bin_boundaries = np.linspace(0, 1, n_bins + 1)
        ece = 0
        
        for i in range(n_bins):
            lower = bin_boundaries[i]
            upper = bin_boundaries[i + 1]
            
            in_bin = (confidences >= lower) & (confidences < upper)
            
            if np.sum(in_bin) > 0:
                avg_confidence = np.mean(confidences[in_bin])
                avg_accuracy = np.mean(correctness[in_bin])
                weight = np.sum(in_bin) / len(confidences)
                
                ece += np.abs(avg_confidence - avg_accuracy) * weight
        
        return ece
    
    def plot_reliability_diagram(self, n_bins=10):
        """Reliability Diagram"""
        bin_boundaries = np.linspace(0, 1, n_bins + 1)
        bin_centers = []
        bin_accs = []
        bin_confs = []
        bin_counts = []
        
        for i in range(n_bins):
            lower = bin_boundaries[i]
            upper = bin_boundaries[i + 1]
            
            in_bin = (self.confidences >= lower) & (self.confidences < upper)
            
            if np.sum(in_bin) > 0:
                bin_centers.append((lower + upper) / 2)
                bin_accs.append(np.mean(self.correctness[in_bin]))
                bin_confs.append(np.mean(self.confidences[in_bin]))
                bin_counts.append(np.sum(in_bin))
        
        plt.figure(figsize=(10, 8))
        
        # Gap í‘œì‹œ
        for i in range(len(bin_centers)):
            plt.plot([bin_confs[i], bin_confs[i]], 
                     [bin_confs[i], bin_accs[i]], 
                     'r-', alpha=0.3, linewidth=2)
        
        # ì´ìƒì ì¸ ì„ 
        plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Perfect Calibration')
        
        # ëª¨ë¸ ì„±ëŠ¥
        plt.scatter(bin_confs, bin_accs, s=np.array(bin_counts)*10, 
                    alpha=0.7, label='Model', color='blue')
        
        plt.xlabel('Confidence', fontsize=12)
        plt.ylabel('Accuracy', fontsize=12)
        plt.title('Reliability Diagram', fontsize=14)
        plt.legend(fontsize=10)
        plt.grid(True, alpha=0.3)
        plt.xlim(0, 1)
        plt.ylim(0, 1)
        plt.tight_layout()
        plt.savefig('outputs/evaluation/reliability_diagram.png', dpi=300)
        plt.close()
    
    def evaluate_per_person(self):
        """ì¸ë¬¼ë³„ ROC/PR í‰ê°€"""
        print("\n[3] Per-Person Evaluation")
        print("-"*70)
        
        # ë“±ë¡ëœ ì¸ë¬¼ ëª©ë¡
        persons = sorted(set(self.y_true) - {'unknown'})
        
        for person_id in persons:
            # Binary ë³€í™˜
            y_binary = [1 if label == person_id else 0 for label in self.y_true]
            
            # í•´ë‹¹ ì¸ë¬¼ì˜ ìœ ì‚¬ë„ ì¶”ì¶œ
            y_scores = []
            for frame in self.frames:
                pred_info = self.predictions[frame]
                score = pred_info['all_scores'].get(person_id, 0.0)
                y_scores.append(score)
            
            # ROC-AUC
            from sklearn.metrics import roc_curve, auc, average_precision_score
            
            fpr, tpr, _ = roc_curve(y_binary, y_scores)
            roc_auc = auc(fpr, tpr)
            
            # PR
            ap = average_precision_score(y_binary, y_scores)
            
            print(f"{person_id}:")
            print(f"  ROC-AUC: {roc_auc:.3f}")
            print(f"  Average Precision: {ap:.3f}")
    
    def evaluate_confidence_distribution(self):
        """í™•ì‹ ë„ ë¶„í¬ ë¶„ì„"""
        print("\n[4] Confidence Distribution")
        print("-"*70)
        
        bins = [0, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0]
        labels = ['0-30%', '30-40%', '40-50%', '50-60%', 
                  '60-70%', '70-80%', '80-100%']
        
        df = pd.DataFrame({
            'confidence': self.confidences,
            'correct': self.correctness
        })
        
        df['bin'] = pd.cut(df['confidence'], bins=bins, labels=labels)
        
        stats = df.groupby('bin').agg({
            'correct': ['count', 'sum', 'mean']
        })
        
        stats.columns = ['Total', 'Correct', 'Accuracy']
        
        print(stats)
        
        # ì‹œê°í™”
        plt.figure(figsize=(12, 6))
        
        # ì„œë¸Œí”Œë¡¯ 1: ìƒ˜í”Œ ìˆ˜
        plt.subplot(1, 2, 1)
        stats['Total'].plot(kind='bar', color='skyblue')
        plt.ylabel('Count')
        plt.xlabel('Confidence Range')
        plt.title('Sample Distribution')
        plt.xticks(rotation=45)
        
        # ì„œë¸Œí”Œë¡¯ 2: ì •í™•ë„
        plt.subplot(1, 2, 2)
        stats['Accuracy'].plot(kind='bar', color='coral')
        plt.ylabel('Accuracy')
        plt.xlabel('Confidence Range')
        plt.title('Accuracy by Confidence Range')
        plt.ylim(0, 1.0)
        plt.axhline(y=self.threshold, color='r', linestyle='--', 
                    label=f'Threshold ({self.threshold})')
        plt.xticks(rotation=45)
        plt.legend()
        
        plt.tight_layout()
        plt.savefig('outputs/evaluation/confidence_distribution.png', dpi=300)
        plt.close()
        
        print("âœ… Confidence distribution saved")

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    evaluator = FaceWatchEvaluator(
        ground_truth_path="outputs/evaluation/ground_truth.json",
        predictions_path="outputs/evaluation/predictions.json",
        threshold=0.42
    )
    
    evaluator.evaluate_all()
```

---

## 5. ì„ê³„ê°’ ìµœì í™”

### 5.1 ì—¬ëŸ¬ ì„ê³„ê°’ ë¹„êµ

```python
def find_optimal_threshold(ground_truth_path, predictions_path):
    """
    ì—¬ëŸ¬ ì„ê³„ê°’ì—ì„œ F1-Score ê³„ì‚°í•˜ì—¬ ìµœì ê°’ ì°¾ê¸°
    """
    thresholds = np.arange(0.30, 0.70, 0.02)
    results = []
    
    for thresh in thresholds:
        evaluator = FaceWatchEvaluator(
            ground_truth_path, predictions_path, threshold=thresh
        )
        
        # F1 ê³„ì‚°
        from sklearn.metrics import f1_score
        f1_macro = f1_score(evaluator.y_true, evaluator.y_pred, 
                            average='macro', zero_division=0)
        f1_weighted = f1_score(evaluator.y_true, evaluator.y_pred, 
                               average='weighted', zero_division=0)
        
        # ECE ê³„ì‚°
        ece = evaluator.compute_ece(evaluator.confidences, evaluator.correctness)
        
        results.append({
            'threshold': thresh,
            'f1_macro': f1_macro,
            'f1_weighted': f1_weighted,
            'ece': ece
        })
        
        print(f"Threshold {thresh:.2f}: F1={f1_macro:.3f}, ECE={ece:.4f}")
    
    # ê²°ê³¼ í”Œë¡¯
    df = pd.DataFrame(results)
    
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    plt.plot(df['threshold'], df['f1_macro'], 'o-', label='F1 (Macro)')
    plt.plot(df['threshold'], df['f1_weighted'], 's-', label='F1 (Weighted)')
    plt.xlabel('Threshold')
    plt.ylabel('F1-Score')
    plt.title('F1-Score vs Threshold')
    plt.legend()
    plt.grid(True)
    
    plt.subplot(1, 2, 2)
    plt.plot(df['threshold'], df['ece'], 'o-', color='red')
    plt.xlabel('Threshold')
    plt.ylabel('ECE')
    plt.title('Calibration Error vs Threshold')
    plt.grid(True)
    
    plt.tight_layout()
    plt.savefig('outputs/evaluation/threshold_optimization.png', dpi=300)
    plt.show()
    
    # ìµœì  ì„ê³„ê°’
    best_f1_idx = df['f1_macro'].idxmax()
    best_ece_idx = df['ece'].idxmin()
    
    print("\nìµœì  ì„ê³„ê°’:")
    print(f"  F1 ìµœëŒ€: {df.loc[best_f1_idx, 'threshold']:.2f} "
          f"(F1={df.loc[best_f1_idx, 'f1_macro']:.3f})")
    print(f"  ECE ìµœì†Œ: {df.loc[best_ece_idx, 'threshold']:.2f} "
          f"(ECE={df.loc[best_ece_idx, 'ece']:.4f})")
    
    return df
```

---

## 6. ê²°ê³¼ í•´ì„

### 6.1 ì§€í‘œ í•´ì„ ê°€ì´ë“œ

#### ECE (Expected Calibration Error)

| ECE ê°’ | í•´ì„ | ì¡°ì¹˜ |
|--------|------|------|
| **< 0.05** | ë§¤ìš° ì˜ ë³´ì •ë¨ | âœ… í˜„ì¬ ì„¤ì • ìœ ì§€ |
| **0.05~0.10** | ì˜ ë³´ì •ë¨ | âœ… ì–‘í˜¸ |
| **0.10~0.20** | ì ë‹¹í•œ ë³´ì • | âš ï¸ ì„ê³„ê°’ ì¡°ì • ê³ ë ¤ |
| **> 0.20** | ê³¼ì‹ /ê³¼ì†Œì‹  | âŒ ë³´ì • í•„ìš” |

#### ROC-AUC

| AUC ê°’ | í•´ì„ |
|--------|------|
| **0.9~1.0** | íƒì›” |
| **0.8~0.9** | ìš°ìˆ˜ |
| **0.7~0.8** | ì–‘í˜¸ |
| **0.6~0.7** | ë³´í†µ |
| **< 0.6** | ê°œì„  í•„ìš” |

### 6.2 ì¼ë°˜ì ì¸ ë¬¸ì œì™€ í•´ê²°ì±…

#### ë¬¸ì œ 1: ECEê°€ ë†’ìŒ (ê³¼ì‹ )

**ì¦ìƒ**: ëª¨ë¸ì´ "90%"ë¼ê³  ì˜ˆì¸¡í–ˆëŠ”ë° ì‹¤ì œë¡œëŠ” 60%ë§Œ ë§ìŒ

**ì›ì¸**:
- ì„ê³„ê°’ì´ ë„ˆë¬´ ë‚®ìŒ
- Bankê°€ í¸í–¥ë¨ (íŠ¹ì • ê°ë„ë§Œ ë§ìŒ)

**í•´ê²°ì±…**:
1. ì„ê³„ê°’ ìƒí–¥ (0.42 â†’ 0.45)
2. Gap margin ì¦ê°€ (0.12 â†’ 0.15)
3. Dynamic Bank ì¬ìˆ˜ì§‘ (ë” ë‹¤ì–‘í•œ ê°ë„)

#### ë¬¸ì œ 2: Recallì´ ë‚®ìŒ (ë¯¸íƒ)

**ì¦ìƒ**: ì‹¤ì œë¡œ yhì¸ë° "unknown"ìœ¼ë¡œ ë¶„ë¥˜

**ì›ì¸**:
- ì„ê³„ê°’ì´ ë„ˆë¬´ ë†’ìŒ
- Bankì— í•´ë‹¹ ê°ë„ ì„ë² ë”© ë¶€ì¡±

**í•´ê²°ì±…**:
1. ì„ê³„ê°’ í•˜í–¥ (0.42 â†’ 0.38)
2. íŠ¹ì • ê°ë„ ì„ë² ë”© ì¶”ê°€ ìˆ˜ì§‘

#### ë¬¸ì œ 3: Precisionì´ ë‚®ìŒ (ì˜¤íƒ)

**ì¦ìƒ**: ì‹¤ì œë¡œ jaì¸ë° "yh"ë¡œ ë¶„ë¥˜

**ì›ì¸**:
- yhì™€ jaì˜ ì„ë² ë”©ì´ ë„ˆë¬´ ë¹„ìŠ·í•¨
- Gap marginì´ ë„ˆë¬´ ì‘ìŒ

**í•´ê²°ì±…**:
1. Gap margin ì¦ê°€
2. Base Bank í’ˆì§ˆ ê°œì„  (ë” ë‚˜ì€ ë“±ë¡ ì‚¬ì§„)

---

## 7. ì‹¤ì „ ì˜ˆì‹œ

### 7.1 ì „ì²´ ì›Œí¬í”Œë¡œìš°

```bash
# 1. í”„ë ˆì„ ì¶”ì¶œ
python src/face_match_cctv.py
# â†’ outputs/results/test_video/frames/

# 2. Ground Truth ë¼ë²¨ë§
python scripts/label_ground_truth.py
# â†’ outputs/evaluation/ground_truth.json

# 3. ëª¨ë¸ ì˜ˆì¸¡ ìˆ˜ì§‘
python scripts/collect_predictions.py
# â†’ outputs/evaluation/predictions.json

# 4. í‰ê°€ ì‹¤í–‰
python scripts/evaluate_model.py
# â†’ outputs/evaluation/*.png

# 5. ì„ê³„ê°’ ìµœì í™”
python scripts/find_optimal_threshold.py
```

### 7.2 ì˜ˆì‹œ ì¶œë ¥

```
======================================================================
FaceWatch ëª¨ë¸ í‰ê°€
======================================================================
í”„ë ˆì„ ìˆ˜: 150
Threshold: 0.42

[1] Classification Metrics
----------------------------------------------------------------------
              precision    recall  f1-score   support

          ja       0.85      0.88      0.86        32
          js       0.78      0.75      0.76        28
          jw       0.82      0.80      0.81        25
          yh       0.91      0.93      0.92        45
     unknown       0.88      0.85      0.86        20

    accuracy                           0.86       150
   macro avg       0.85      0.84      0.84       150
weighted avg       0.86      0.86      0.86       150

âœ… Confusion matrix saved

[2] Calibration Metrics
----------------------------------------------------------------------
Expected Calibration Error (ECE): 0.0723
  â†’ ì˜ ë³´ì •ë¨ âœ…
âœ… Reliability diagram saved

[3] Per-Person Evaluation
----------------------------------------------------------------------
ja:
  ROC-AUC: 0.921
  Average Precision: 0.875
js:
  ROC-AUC: 0.902
  Average Precision: 0.843
jw:
  ROC-AUC: 0.915
  Average Precision: 0.868
yh:
  ROC-AUC: 0.948
  Average Precision: 0.923

[4] Confidence Distribution
----------------------------------------------------------------------
                Total  Correct  Accuracy
bin                                     
0-30%              15       10  0.666667
30-40%             20       16  0.800000
40-50%             18       15  0.833333
50-60%             22       20  0.909091
60-70%             28       26  0.928571
70-80%             25       24  0.960000
80-100%            22       22  1.000000
âœ… Confidence distribution saved

======================================================================
í‰ê°€ ì™„ë£Œ!
ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: outputs/evaluation
======================================================================
```

---

## 8. ì²´í¬ë¦¬ìŠ¤íŠ¸

### í‰ê°€ ì „ ì¤€ë¹„

- [ ] Ground Truth ë¼ë²¨ë§ ì™„ë£Œ
- [ ] ëª¨ë¸ ì˜ˆì¸¡ ìˆ˜ì§‘ ì™„ë£Œ
- [ ] í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ ì„¤ì¹˜ (`pip install scikit-learn matplotlib seaborn pandas`)

### í•„ìˆ˜ í‰ê°€ í•­ëª©

- [ ] Confusion Matrix í™•ì¸
- [ ] Precision/Recall/F1 ê³„ì‚°
- [ ] ECE ê³„ì‚° (ì‹ ë¢°ë„ ë³´ì •)
- [ ] Reliability Diagram ì‹œê°í™”
- [ ] ì¸ë¬¼ë³„ ROC-AUC ê³„ì‚°
- [ ] í™•ì‹ ë„ ë¶„í¬ ë¶„ì„

### ì„ê³„ê°’ ìµœì í™”

- [ ] ì—¬ëŸ¬ ì„ê³„ê°’ì—ì„œ F1 ë¹„êµ
- [ ] ECE vs Threshold í”Œë¡¯
- [ ] ìµœì  ì„ê³„ê°’ ì„ ì •
- [ ] ì‹¤ì„œë¹„ìŠ¤ì— ì ìš©

---

## ë¶€ë¡

### A. í‰ê°€ ì§€í‘œ ìš”ì•½

| ì§€í‘œ | ëª©ì  | ì„ê³„ê°’ ì˜í–¥ | ì¶”ì²œë„ |
|------|------|------------|--------|
| **ECE** | ì‹ ë¢°ë„ ë³´ì • | ìˆìŒ | â­â­â­â­â­ |
| **Precision/Recall/F1** | ì „ì²´ ì„±ëŠ¥ | ìˆìŒ | â­â­â­â­ |
| **ROC-AUC** | ì„ê³„ê°’ ë¬´ê´€ ì„±ëŠ¥ | ì—†ìŒ | â­â­â­â­ |
| **Confusion Matrix** | ì˜¤ë¥˜ íŒ¨í„´ ë¶„ì„ | ìˆìŒ | â­â­â­â­ |
| **Reliability Diagram** | ë³´ì • ì‹œê°í™” | ìˆìŒ | â­â­â­ |
| **í¬ë¡œìŠ¤ì—”íŠ¸ë¡œí”¼** | (ë¶€ì í•©) | - | âŒ |

### B. ì°¸ê³  ë¬¸í—Œ

- Guo et al., "On Calibration of Modern Neural Networks", ICML 2017
- Naeini et al., "Obtaining Well Calibrated Probabilities Using Bayesian Binning", AAAI 2015

---

**ë¬¸ì„œ ë²„ì „**: 1.0  
**ìµœì¢… ìˆ˜ì •**: 2025-11-27  
**ì‘ì„±ì**: FaceWatch Development Team
