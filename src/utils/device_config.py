# src/utils/device_config.py
"""
GPU/CPU ë””ë°”ì´ìŠ¤ ì„¤ì • ëª¨ë“ˆ
GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ê³  ì ì ˆí•œ ctx_idë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
"""
import os
import sys
import warnings
import onnxruntime as ort
from typing import Optional
from pathlib import Path

def _find_cuda_path() -> Optional[str]:
    """
    ì‹œìŠ¤í…œì— ì„¤ì¹˜ëœ CUDA ê²½ë¡œë¥¼ ì°¾ìŠµë‹ˆë‹¤.
    
    Returns:
        str: CUDA bin ê²½ë¡œ (ì—†ìœ¼ë©´ None)
    """
    possible_paths = [
        r"C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.4\bin",
        r"C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.3\bin",
        r"C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.2\bin",
        r"C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.1\bin",
        r"C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.0\bin",
        r"C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.8\bin",
        r"C:\Program Files (x86)\NVIDIA GPU Computing Toolkit\CUDA\v12.0\bin",
        r"C:\Program Files (x86)\NVIDIA GPU Computing Toolkit\CUDA\v11.8\bin",
    ]
    
    # í™˜ê²½ ë³€ìˆ˜ì—ì„œë„ í™•ì¸
    cuda_path = os.getenv("CUDA_PATH")
    if cuda_path:
        bin_path = Path(cuda_path) / "bin"
        if bin_path.exists():
            return str(bin_path)
    
    # ì¼ë°˜ì ì¸ ì„¤ì¹˜ ê²½ë¡œ í™•ì¸
    for path in possible_paths:
        if Path(path).exists():
            # cublasLt64_12.dll ê°™ì€ í•„ìˆ˜ DLLì´ ìžˆëŠ”ì§€ í™•ì¸
            dll_files = list(Path(path).glob("cublasLt*.dll"))
            if dll_files:
                return path
    
    return None

def _ensure_cuda_in_path() -> bool:
    """
    CUDA ê²½ë¡œê°€ PATHì— ì—†ìœ¼ë©´ ì¶”ê°€í•©ë‹ˆë‹¤.
    
    Returns:
        bool: CUDA ê²½ë¡œë¥¼ ì°¾ì•„ì„œ ì¶”ê°€í–ˆìœ¼ë©´ True
    """
    cuda_path = _find_cuda_path()
    if not cuda_path:
        return False
    
    current_path = os.getenv("PATH", "")
    if cuda_path not in current_path:
        os.environ["PATH"] = f"{cuda_path};{current_path}"
        return True
    
    return False

# ëª¨ë“ˆ ë¡œë“œ ì‹œ ìžë™ìœ¼ë¡œ CUDA ê²½ë¡œ ì¶”ê°€ ì‹œë„
_ensure_cuda_in_path()

def get_device_id() -> int:
    """
    GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ê³  ì ì ˆí•œ ë””ë°”ì´ìŠ¤ IDë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì‹¤ì œ CUDA ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ê°€ëŠ¥ ì—¬ë¶€ë„ í™•ì¸í•©ë‹ˆë‹¤.
    
    Returns:
        int: GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ 0, ì•„ë‹ˆë©´ -1 (CPU)
    """
    # í™˜ê²½ ë³€ìˆ˜ë¡œ ê°•ì œ ì„¤ì • ê°€ëŠ¥
    force_cpu = os.getenv("FORCE_CPU", "0").lower() in ("1", "true", "yes")
    if force_cpu:
        return -1
    
    # GPU ì¸ë±ìŠ¤ ì§€ì • ê°€ëŠ¥ (ê¸°ë³¸ê°’: 0)
    gpu_index = int(os.getenv("GPU_INDEX", "0"))
    
    try:
        # onnxruntimeì˜ ì—ëŸ¬ ë©”ì‹œì§€ ì–µì œ
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore", category=UserWarning)
            
            providers = ort.get_available_providers()
            
            # Providerê°€ ìžˆì–´ë„ ì‹¤ì œë¡œ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸
            if 'CUDAExecutionProvider' in providers or 'TensorrtExecutionProvider' in providers:
                # ì‹¤ì œ GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ëŠ” InsightFaceê°€ ìžë™ìœ¼ë¡œ ì²˜ë¦¬í•˜ë¯€ë¡œ
                # ì—¬ê¸°ì„œëŠ” provider ì¡´ìž¬ ì—¬ë¶€ë§Œ í™•ì¸
                return gpu_index
            else:
                return -1
    except Exception:
        # ì—ëŸ¬ ë°œìƒ ì‹œ ì•ˆì „í•˜ê²Œ CPU ì‚¬ìš©
        return -1

def safe_prepare_insightface(app, device_id: int, det_size: tuple = (640, 640), verbose: bool = True) -> int:
    """
    InsightFaceì˜ prepare()ë¥¼ ì•ˆì „í•˜ê²Œ í˜¸ì¶œí•©ë‹ˆë‹¤.
    GPU ì‚¬ìš© ì‹¤íŒ¨ ì‹œ ìžë™ìœ¼ë¡œ CPUë¡œ fallbackí•©ë‹ˆë‹¤.
    
    Args:
        app: FaceAnalysis ì¸ìŠ¤í„´ìŠ¤
        device_id: ì‚¬ìš©í•˜ë ¤ëŠ” ë””ë°”ì´ìŠ¤ ID (0 ì´ìƒì´ë©´ GPU, -1ì´ë©´ CPU)
        det_size: detection size
        verbose: ìƒì„¸ ë©”ì‹œì§€ ì¶œë ¥ ì—¬ë¶€
    
    Returns:
        int: ì‹¤ì œë¡œ ì‚¬ìš©ëœ ë””ë°”ì´ìŠ¤ ID (GPU ì‹¤íŒ¨ ì‹œ -1 ë°˜í™˜)
    """
    # GPU ì‚¬ìš© ì‹œë„
    if device_id >= 0:
        # CUDA ê²½ë¡œê°€ PATHì— ì—†ìœ¼ë©´ ì¶”ê°€ ì‹œë„
        cuda_added = _ensure_cuda_in_path()
        if cuda_added and verbose:
            cuda_path = _find_cuda_path()
            print(f"ðŸ”§ CUDA ê²½ë¡œë¥¼ PATHì— ì¶”ê°€í–ˆìŠµë‹ˆë‹¤: {cuda_path}")
        
        try:
            # stderr ë¦¬ë‹¤ì´ë ‰íŠ¸ë¡œ ì—ëŸ¬ ë©”ì‹œì§€ ì–µì œ ì‹œë„
            import io
            from contextlib import redirect_stderr
            
            # GPU ì´ˆê¸°í™” ì‹œë„ (ì—ëŸ¬ ë©”ì‹œì§€ëŠ” ì–µì œ)
            with redirect_stderr(io.StringIO()):
                app.prepare(ctx_id=device_id, det_size=det_size)
            
            if verbose:
                print(f"âœ… GPU ì´ˆê¸°í™” ì„±ê³µ (ctx_id={device_id})")
            return device_id
            
        except Exception as e:
            # GPU ì‹¤íŒ¨ ì‹œ CPUë¡œ fallback
            if verbose:
                print(f"âš ï¸ GPU ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)[:100]}")
                print(f"   CPUë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            
            try:
                app.prepare(ctx_id=-1, det_size=det_size)
                if verbose:
                    print(f"âœ… CPU ì´ˆê¸°í™” ì„±ê³µ")
                return -1
            except Exception as e2:
                # CPUë„ ì‹¤íŒ¨í•˜ë©´ ì—ëŸ¬ ë°œìƒ
                raise RuntimeError(f"CPU ì´ˆê¸°í™”ë„ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e2}")
    else:
        # CPU ì‚¬ìš©
        app.prepare(ctx_id=-1, det_size=det_size)
        if verbose:
            print(f"âœ… CPU ì´ˆê¸°í™” ì„±ê³µ")
        return -1

def get_device_info() -> dict:
    """
    í˜„ìž¬ ë””ë°”ì´ìŠ¤ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Returns:
        dict: ë””ë°”ì´ìŠ¤ ì •ë³´ (device_id, device_type, providers)
    """
    device_id = get_device_id()
    
    try:
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore")
            providers = ort.get_available_providers()
    except Exception:
        providers = []
    
    device_type = "GPU" if device_id >= 0 else "CPU"
    
    return {
        "device_id": device_id,
        "device_type": device_type,
        "providers": providers
    }

